import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import torch.optim as optim
import torch.nn.functional as F
from tqdm import tqdm
import matplotlib.patches as patches
import rl_utils
import scipy.special
import time

# ğŸŒŸ 1. å¯¼å…¥é…ç½® å’Œ TensorBoard
import config
from torch.utils.tensorboard import SummaryWriter

matplotlib.use("TkAgg")


# ---------------------------------------------------------------------
# ğŸŒŸ è¾…åŠ©ç±»: Rollout ç¼“å†²åŒº (N-Step æ›´æ–°æ‰€å¿…éœ€)
# (ä¸ä¹‹å‰ç›¸åŒ)
# ---------------------------------------------------------------------
class RolloutBuffer:
    def __init__(self, buffer_size, state_dim, hidden_dim, device):
        self.buffer_size = buffer_size
        self.state_dim = state_dim
        self.hidden_dim = hidden_dim
        self.device = device
        self.clear()

    def clear(self):
        # å­˜å‚¨ N æ­¥çš„æ•°æ®
        self.states = torch.zeros((self.buffer_size, self.state_dim), dtype=torch.float32)
        self.actions = torch.zeros((self.buffer_size, 1), dtype=torch.int64)
        self.log_probs = torch.zeros((self.buffer_size, 1), dtype=torch.float32)
        self.rewards = torch.zeros((self.buffer_size, 1), dtype=torch.float32)
        self.dones = torch.zeros((self.buffer_size, 1), dtype=torch.float32)
        self.values = torch.zeros((self.buffer_size, 1), dtype=torch.float32)
        # ğŸŒŸ å¿…é¡»å­˜å‚¨ RNN çš„éšè—çŠ¶æ€
        self.h_actor = torch.zeros((self.buffer_size, self.hidden_dim), dtype=torch.float32)

        self.ptr = 0  # ç¼“å†²åŒºæŒ‡é’ˆ

    def add(self, state, action, log_prob, reward, done, value, h_actor):
        if self.ptr >= self.buffer_size:
            print("Warning: Rollout buffer overflow")
            return

        self.states[self.ptr] = torch.as_tensor(state, device='cpu')
        self.actions[self.ptr] = torch.as_tensor([action], device='cpu')
        self.log_probs[self.ptr] = torch.as_tensor([log_prob], device='cpu')
        self.rewards[self.ptr] = torch.as_tensor([reward], device='cpu')
        self.dones[self.ptr] = torch.as_tensor([done], device='cpu')
        self.values[self.ptr] = torch.as_tensor([value], device='cpu')
        self.h_actor[self.ptr] = h_actor.squeeze(0).to('cpu')

        self.ptr += 1

    def compute_returns_and_advantages(self, last_value, gamma, lmbda):
        """è®¡ç®— GAE (å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡)"""
        last_value = last_value.to('cpu')
        last_gae_lam = 0

        # æˆ‘ä»¬éœ€è¦ advantages å’Œ returns
        self.advantages = torch.zeros_like(self.rewards)
        self.returns = torch.zeros_like(self.rewards)

        for t in reversed(range(self.buffer_size)):
            if t == self.buffer_size - 1:
                next_non_terminal = 1.0 - self.dones[t]  # æ£€æŸ¥æœ€åä¸€æ­¥æ˜¯ä¸æ˜¯ done
                next_values = last_value
            else:
                next_non_terminal = 1.0 - self.dones[t + 1]
                next_values = self.values[t + 1]

            # GAE è®¡ç®—
            delta = self.rewards[t] + gamma * next_values * next_non_terminal - self.values[t]
            self.advantages[t] = last_gae_lam = delta + gamma * lmbda * next_non_terminal * last_gae_lam

        # Returns = Advantages + Values
        self.returns = self.advantages + self.values

    def get_batches(self, minibatch_size):
        """ä¸º RNN åˆ›å»ºé¡ºåºçš„ mini-batch"""
        # N-Step æ›´æ–°: æˆ‘ä»¬å°†æ•´ä¸ª 2048 æ­¥çš„æ•°æ®åˆ†æˆ N å—
        # (è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œæ²¡æœ‰å¤„ç†è·¨ episode è¾¹ç•Œ)
        num_minibatches = self.buffer_size // minibatch_size

        # å°†æ‰€æœ‰æ•°æ®å‘é€åˆ° device
        self.states = self.states.to(self.device)
        self.actions = self.actions.to(self.device)
        self.log_probs = self.log_probs.to(self.device)
        self.advantages = self.advantages.to(self.device)
        self.returns = self.returns.to(self.device)
        self.h_actor = self.h_actor.to(self.device)

        indices = np.arange(self.buffer_size)
        # æ³¨æ„ï¼šä¸ºäº† RNNï¼Œæˆ‘ä»¬ä¸åº”è¯¥å®Œå…¨éšæœºæ‰“ä¹± (shuffle)
        # æˆ‘ä»¬åœ¨è¿™é‡Œâ€œéšæœºâ€é€‰æ‹©èµ·å§‹ç‚¹ï¼Œä½†ä¿æŒ minibatch å†…éƒ¨çš„é¡ºåºæ€§
        # (è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„ä¸»é¢˜ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ç®€åŒ–çš„â€œéšæœºé¡ºåºçš„é¡ºåºå—â€)
        np.random.shuffle(indices.reshape(-1, minibatch_size))

        for start in range(0, self.buffer_size, minibatch_size):
            end = start + minibatch_size
            batch_indices = indices[start:end]

            yield (
                self.states[batch_indices],
                self.actions[batch_indices],
                self.log_probs[batch_indices],
                self.advantages[batch_indices],
                self.returns[batch_indices],
                self.h_actor[batch_indices[0]].unsqueeze(0)  # ğŸŒŸ å–è¯¥å—çš„ç¬¬ä¸€ä¸ªéšè—çŠ¶æ€ä½œä¸ºåˆå§‹çŠ¶æ€
            )


# ---------------------------------------------------------------------
# --- è¾…åŠ©å‡½æ•°, éšœç¢ç‰©, çŠ¶æ€è®¡ç®— (ä¸å˜, å¯¼å…¥ config) ---
# ---------------------------------------------------------------------
def wrap_to_pi(angle):
    return (angle + np.pi) % (2 * np.pi) - np.pi


def update_movement(x, y, theta, v_D, v_A, DT):
    theta += v_A * DT
    theta = wrap_to_pi(theta)
    x += v_D * np.cos(theta) * DT
    y += v_D * np.sin(theta) * DT
    vx = v_D * np.cos(theta)
    vy = v_D * np.sin(theta)
    return x, y, theta, vx, vy


class Obstacle:
    def __init__(self, x, y, vx, vy, speed_range=(0.1, 0.3)):
        self.x_init, self.y_init = x, y
        self.vx_init, self.vy_init = vx, vy
        self.speed_range = speed_range
        self.reset()

    def reset(self, x_init=None, y_init=None):
        if x_init is not None and y_init is not None:
            self.x, self.y = x_init, y_init
        else:
            self.x = np.random.uniform(config.OBSTACLE_SPAWN_BOX[0], config.OBSTACLE_SPAWN_BOX[1])
            self.y = np.random.uniform(config.OBSTACLE_SPAWN_BOX[2], config.OBSTACLE_SPAWN_BOX[3])
        angle = np.random.uniform(0, 2 * np.pi)
        speed = np.random.uniform(*self.speed_range)
        self.vx = speed * np.cos(angle)
        self.vy = speed * np.sin(angle)

    def update(self):
        self.x += self.vx * config.DT
        self.y += self.vy * config.DT
        if not (config.SPAWN_BOX[0] < self.x < config.SPAWN_BOX[1]):
            self.vx *= -1
        if not (config.SPAWN_BOX[2] < self.y < config.SPAWN_BOX[3]):
            self.vy *= -1


def calculate_state(robot_x, robot_y, robot_theta, robot_vx, robot_vy,
                    target_x, target_y, obstacles):
    dx_goal = target_x - robot_x
    dy_goal = target_y - robot_y
    goal_dist = np.hypot(dx_goal, dy_goal)
    angle_to_goal = np.arctan2(dy_goal, dx_goal)
    relative_goal_angle = wrap_to_pi(angle_to_goal - robot_theta)
    goal_cos = np.cos(relative_goal_angle)
    goal_sin = np.sin(relative_goal_angle)
    state = [goal_dist, goal_cos, goal_sin]
    cos_theta = np.cos(-robot_theta)
    sin_theta = np.sin(-robot_theta)
    for obs in obstacles:
        rel_x_world = obs.x - robot_x
        rel_y_world = obs.y - robot_y
        rel_vx_world = obs.vx - robot_vx
        rel_vy_world = obs.vy - robot_vy
        rel_x_robot = rel_x_world * cos_theta - rel_y_world * sin_theta
        rel_y_robot = rel_x_world * sin_theta + rel_y_world * cos_theta
        rel_vx_robot = rel_vx_world * cos_theta - rel_vy_world * sin_theta
        rel_vy_robot = rel_vx_world * sin_theta + rel_vy_world * cos_theta
        state.extend([rel_x_robot, rel_y_robot, rel_vx_robot, rel_vy_robot])
    return np.array(state), relative_goal_angle


# ---------------------------------------------------------------------
# ğŸŒŸ Actor-Critic ç½‘ç»œ (ä¸å˜)
# ---------------------------------------------------------------------
class ActorGRU(nn.Module):
    def __init__(self, input_dim, hidden_dim, action_dim):
        super(ActorGRU, self).__init__()
        self.hidden_dim = hidden_dim
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.gru_cell = nn.GRUCell(hidden_dim, hidden_dim)
        self.action_head = nn.Linear(hidden_dim, action_dim)

    def forward(self, x, hidden_state):
        x = F.relu(self.fc1(x))
        next_hidden_state = self.gru_cell(x, hidden_state)
        logits = self.action_head(next_hidden_state)
        dist = torch.distributions.Categorical(logits=logits)
        return dist, next_hidden_state


class CriticGRU(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(CriticGRU, self).__init__()
        self.hidden_dim = hidden_dim
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.gru_cell = nn.GRUCell(hidden_dim, hidden_dim)
        self.value_head = nn.Linear(hidden_dim, 1)

    def forward(self, x, hidden_state):
        x = F.relu(self.fc1(x))
        next_hidden_state = self.gru_cell(x, hidden_state)
        value = self.value_head(next_hidden_state)
        return value, next_hidden_state


# ---------------------------------------------------------------------
# ğŸŒŸ PPO ç®—æ³•ç±» (é‡æ„ä»¥æ”¯æŒ N-Step æ›´æ–°)
# (ä¸ä¹‹å‰ç›¸åŒ)
# ---------------------------------------------------------------------
class PPO_GRU:
    def __init__(self, state_dim, action_dim, gru_hidden_dim,
                 actor_lr, critic_lr, lmbda, epochs, eps, gamma, device):
        self.device = device
        self.gamma = gamma
        self.lmbda = lmbda
        self.epochs = epochs
        self.eps = eps

        self.actor = ActorGRU(state_dim, gru_hidden_dim, action_dim).to(self.device)
        self.critic = CriticGRU(state_dim, gru_hidden_dim).to(self.device)

        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)

    def take_action(self, state, actor_hidden):
        """(ç”¨äº Rollout) - è¾“å‡ºå¸¦æ¢¯åº¦çš„åŠ¨ä½œ"""
        state = torch.as_tensor(state, dtype=torch.float).to(self.device).unsqueeze(0)

        # è¯„ä¼° Actor
        self.actor.eval()  # ç¡®ä¿åœ¨ rollout æ—¶å¤„äº eval æ¨¡å¼
        with torch.no_grad():
            dist, next_actor_hidden = self.actor(state, actor_hidden)
        self.actor.train()  # åˆ‡æ¢å› train æ¨¡å¼

        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob.item(), next_actor_hidden

    def get_value(self, state, critic_hidden):
        """(ç”¨äº Rollout) - è·å–å½“å‰çŠ¶æ€çš„ä»·å€¼"""
        state = torch.as_tensor(state, dtype=torch.float).to(self.device).unsqueeze(0)

        self.critic.eval()
        with torch.no_grad():
            value, next_critic_hidden = self.critic(state, critic_hidden)
        self.critic.train()

        return value.item(), next_critic_hidden

    def update(self, buffer, writer, global_step):
        """
        N-Step æ›´æ–°ï¼š
        ä½¿ç”¨ N æ­¥çš„ç¼“å†²åŒºæ•°æ®, è®­ç»ƒ K ä¸ª Epoch, æ¯ä¸ª Epoch åˆ† M ä¸ª Mini-Batch
        """

        # å­˜å‚¨æŸå¤±ä»¥ä¾¿ TensorBoard è®°å½•
        actor_losses = []
        critic_losses = []
        entropies = []

        for _ in range(self.epochs):
            for batch in buffer.get_batches(config.MINIBATCH_SIZE):
                (
                    mb_states,
                    mb_actions,
                    mb_old_log_probs,
                    mb_advantages,
                    mb_returns,
                    mb_h_actor_initial,  # åˆå§‹éšè—çŠ¶æ€
                ) = batch

                # --- é‡æ–°è®¡ç®— Actor (é‡æ”¾) ---
                # æˆ‘ä»¬éœ€è¦é‡æ”¾æ•´ä¸ª mini-batch åºåˆ—
                T = len(mb_states)
                new_log_probs = []
                new_entropies = []
                h_a = mb_h_actor_initial
                for t in range(T):
                    dist, h_a = self.actor(mb_states[t].unsqueeze(0), h_a)
                    new_log_probs.append(dist.log_prob(mb_actions[t]))
                    new_entropies.append(dist.entropy())

                new_log_probs = torch.stack(new_log_probs)
                entropy_loss = torch.stack(new_entropies).mean()

                # --- é‡æ–°è®¡ç®— Critic (é‡æ”¾) ---
                h_c = mb_h_actor_initial.detach()  # å‡è®¾ Actor/Critic å…±äº«çŠ¶æ€
                new_values = []
                for t in range(T):
                    v, h_c = self.critic(mb_states[t].unsqueeze(0), h_c)
                    new_values.append(v.squeeze(0))
                new_values = torch.stack(new_values)

                # --- PPO æŸå¤±è®¡ç®— ---
                ratio = torch.exp(new_log_probs - mb_old_log_probs)
                surr1 = ratio * mb_advantages
                surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * mb_advantages

                actor_loss = -torch.min(surr1, surr2).mean()
                critic_loss = F.mse_loss(new_values, mb_returns)

                loss = (actor_loss +
                        0.5 * critic_loss -
                        config.ENTROPY_COEF * entropy_loss)

                # --- æ¢¯åº¦æ›´æ–° ---
                self.actor_optimizer.zero_grad()
                self.critic_optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(self.actor.parameters(), config.GRAD_CLIP_MAX_NORM)
                nn.utils.clip_grad_norm_(self.critic.parameters(), config.GRAD_CLIP_MAX_NORM)
                self.actor_optimizer.step()
                self.critic_optimizer.step()

                actor_losses.append(actor_loss.item())
                critic_losses.append(critic_loss.item())
                entropies.append(entropy_loss.item())

        # ğŸŒŸ è®°å½•æ—¥å¿— (å¹³å‡å€¼)
        writer.add_scalar("Loss/Actor_Loss", np.mean(actor_losses), global_step)
        writer.add_scalar("Loss/Critic_Loss", np.mean(critic_losses), global_step)
        writer.add_scalar("Metrics/Entropy", np.mean(entropies), global_step)


# --- å®ä¾‹åŒ– Agent å’Œç¯å¢ƒ ---
# ğŸŒŸ å…¨éƒ¨ä½¿ç”¨ config
device = config.DEVICE
agent = PPO_GRU(config.STATE_DIM, config.ACTION_DIM, config.GRU_HIDDEN_DIM,
                config.ACTOR_LR, config.CRITIC_LR, config.LMBDA, config.EPOCHS,
                config.EPS, config.GAMMA, device)

buffer = RolloutBuffer(config.ROLLOUT_STEPS, config.STATE_DIM, config.GRU_HIDDEN_DIM, device)

obstacles = []
for _ in range(config.NUM_RANDOM_OBSTACLES):
    obstacles.append(Obstacle(0, 0, 0, 0))

# ğŸŒŸ åˆå§‹åŒ– TensorBoard
writer = SummaryWriter(config.LOG_DIR)
print(f"Logging to {config.LOG_DIR}, using device {device}")
print(f"Total timesteps: {config.TOTAL_TIMESTEPS}, Rollout size: {config.ROLLOUT_STEPS}")

# ---------------------------------------------------------------------
# ğŸŒŸ æ­¥éª¤ 4: PPO è®­ç»ƒä¸»å¾ªç¯ (N-Step ç‰ˆæœ¬)
# ---------------------------------------------------------------------

# --- åˆå§‹åŒ–ç¯å¢ƒçŠ¶æ€ ---
start_x = np.random.uniform(config.SPAWN_BOX[0], config.SPAWN_BOX[1])
start_y = np.random.uniform(config.SPAWN_BOX[2], config.SPAWN_BOX[3])
start_yaw = np.random.uniform(-np.pi, np.pi)
target_x = np.random.uniform(config.SPAWN_BOX[0], config.SPAWN_BOX[1])
target_y = np.random.uniform(config.SPAWN_BOX[2], config.SPAWN_BOX[3])
for obs in obstacles:
    obs.reset()
    while np.hypot(obs.x - start_x, obs.y - start_y) < config.R_OBSTACLE * 4:
        obs.reset()

x, y, theta = start_x, start_y, start_yaw
vx, vy = 0.0, 0.0
state, _ = calculate_state(x, y, theta, vx, vy, target_x, target_y, obstacles)

# --- åˆå§‹åŒ– RNN éšè—çŠ¶æ€ ---
actor_hidden = torch.zeros(1, config.GRU_HIDDEN_DIM).to(device)
critic_hidden = torch.zeros(1, config.GRU_HIDDEN_DIM).to(device)  # æˆ‘ä»¬éœ€è¦å®ƒæ¥è·å– value

# --- åˆå§‹åŒ–æ—¥å¿—è¿½è¸ªå™¨ ---
global_step = 0
num_updates = config.TOTAL_TIMESTEPS // config.ROLLOUT_STEPS
start_time = time.time()

# --- ä¸»è®­ç»ƒå¾ªç¯ ---
for update_num in tqdm(range(1, num_updates + 1)):

    # æ¸…ç©ºç¼“å†²åŒºï¼Œå‡†å¤‡æ”¶é›† N æ­¥æ•°æ®
    buffer.clear()

    # ä¸´æ—¶è¿½è¸ªå™¨ï¼Œç”¨äºè®°å½• rollout æœŸé—´çš„å¥–åŠ±
    ep_rewards = []
    ep_successes = []
    ep_lengths = []
    # ğŸŒŸ ä¿®å¤ 1: æ·»åŠ æ–°çš„åˆ—è¡¨
    ep_rew_success = []
    ep_rew_collision = []
    ep_rew_shaping = []
    ep_rew_heading = []
    ep_rew_obstacle = []
    ep_rew_step = []

    current_episode_reward = 0
    current_episode_len = 0
    # ğŸŒŸ ä¿®å¤ 2: æ·»åŠ æ–°çš„ç´¯åŠ å™¨
    current_ep_rew_success = 0
    current_ep_rew_collision = 0
    current_ep_rew_shaping = 0
    current_ep_rew_heading = 0
    current_ep_rew_obstacle = 0
    current_ep_rew_step = 0

    # ---------------------------------
    # 1. ROLLOUT (æ•°æ®æ”¶é›†)
    # ---------------------------------
    for step in range(config.ROLLOUT_STEPS):
        global_step += 1
        current_episode_len += 1

        # --- åŠ¨ä½œé€‰æ‹© ---
        action, log_prob, next_actor_hidden = agent.take_action(state, actor_hidden)
        value, next_critic_hidden = agent.get_value(state, critic_hidden)  # å¿…é¡»è·å– value

        v_A = config.A[action]
        v_D = config.D[action]

        # --- ç¯å¢ƒæ­¥è¿› ---
        x, y, theta, vx, vy = update_movement(x, y, theta, v_D, v_A, config.DT)
        for obs in obstacles:
            obs.update()
        next_state, relative_goal_angle = calculate_state(x, y, theta, vx, vy, target_x, target_y, obstacles)

        # --- å¥–åŠ±è®¡ç®— (ä½¿ç”¨ config) ---
        dist_to_target = next_state[0]
        last_goal_dist = state[0]

        done = False
        success = 0

        # 1. æˆåŠŸ
        if dist_to_target < config.TARGET_REACH_THRESH:
            reward_success = config.REWARD_SUCCESS
            success = 1
            done = True
        else:
            reward_success = 0.0

        # 2. ç¢°æ’
        collided = False
        min_dist_to_obs = float('inf')
        for obs in obstacles:
            dist = np.hypot(x - obs.x, y - obs.y)
            min_dist_to_obs = min(min_dist_to_obs, dist)
            if dist < (config.R_OBSTACLE + config.R_ROBOT):
                collided = True
                break
        if collided:
            reward_collision = config.REWARD_COLLISION
            done = True
        else:
            reward_collision = 0.0

        # 3. è¶…æ—¶ (å¦‚æœæ­¥æ•°è¾¾åˆ°ä¸Šé™)
        if current_episode_len >= config.STEPS_PER_EPISODE:
            done = True

        # 4. å…¶ä»–å¥–åŠ±
        reward_shaping = (last_goal_dist - dist_to_target) * config.REWARD_SHAPING_WEIGHT
        reward_heading = config.REWARD_HEADING_WEIGHT * abs(relative_goal_angle)
        if min_dist_to_obs < 0.5:
            reward_obstacle = config.REWARD_OBSTACLE_WEIGHT * (1 - min_dist_to_obs / 0.5)
        else:
            reward_obstacle = 0.0
        reward_step = config.REWARD_STEP_WEIGHT

        # æ€»å¥–åŠ±
        reward = (reward_success + reward_collision + reward_shaping +
                  reward_heading + reward_obstacle + reward_step)

        # ğŸŒŸ ä¿®å¤ 3: åˆ†åˆ«ç´¯åŠ 
        current_ep_rew_success += reward_success
        current_ep_rew_collision += reward_collision
        current_ep_rew_shaping += reward_shaping
        current_ep_rew_heading += reward_heading
        current_ep_rew_obstacle += reward_obstacle
        current_ep_rew_step += reward_step

        current_episode_reward += reward

        # --- å­˜å‚¨åˆ°ç¼“å†²åŒº ---
        buffer.add(state, action, log_prob, reward, done, value, actor_hidden)

        # æ›´æ–°çŠ¶æ€
        state = np.copy(next_state)
        actor_hidden = next_actor_hidden
        critic_hidden = next_critic_hidden  # Critic éšè—çŠ¶æ€ä¹Ÿå¿…é¡»æ›´æ–°

        # --- å¦‚æœ episode ç»“æŸ (Done) ---
        if done:
            # ğŸŒŸ ä¿®å¤ 4a: è®°å½•æ‰€æœ‰åˆ†é¡¹
            ep_rewards.append(current_episode_reward)
            ep_successes.append(success)
            ep_lengths.append(current_episode_len)

            ep_rew_success.append(current_ep_rew_success)
            ep_rew_collision.append(current_ep_rew_collision)
            ep_rew_shaping.append(current_ep_rew_shaping)
            ep_rew_heading.append(current_ep_rew_heading)
            ep_rew_obstacle.append(current_ep_rew_obstacle)
            ep_rew_step.append(current_ep_rew_step)

            # --- é‡ç½®ç¯å¢ƒ ---
            start_x = np.random.uniform(config.SPAWN_BOX[0], config.SPAWN_BOX[1])
            start_y = np.random.uniform(config.SPAWN_BOX[2], config.SPAWN_BOX[3])
            start_yaw = np.random.uniform(-np.pi, np.pi)
            target_x = np.random.uniform(config.SPAWN_BOX[0], config.SPAWN_BOX[1])
            target_y = np.random.uniform(config.SPAWN_BOX[2], config.SPAWN_BOX[3])
            for obs in obstacles:
                obs.reset()

            x, y, theta = start_x, start_y, start_yaw
            vx, vy = 0.0, 0.0
            state, _ = calculate_state(x, y, theta, vx, vy, target_x, target_y, obstacles)

            # ğŸŒŸ é‡ç½® RNN éšè—çŠ¶æ€
            actor_hidden = torch.zeros(1, config.GRU_HIDDEN_DIM).to(device)
            critic_hidden = torch.zeros(1, config.GRU_HIDDEN_DIM).to(device)

            # ğŸŒŸ ä¿®å¤ 4b: é‡ç½®æ‰€æœ‰ç´¯åŠ å™¨
            current_episode_reward = 0
            current_episode_len = 0

            current_ep_rew_success = 0
            current_ep_rew_collision = 0
            current_ep_rew_shaping = 0
            current_ep_rew_heading = 0
            current_ep_rew_obstacle = 0
            current_ep_rew_step = 0

    # ---------------------------------
    # 2. GAE è®¡ç®— ä¸ PPO æ›´æ–°
    # ---------------------------------

    # è·å– N æ­¥ä¸­æœ€åä¸€æ­¥çš„ä»·å€¼, ç”¨äº GAE
    with torch.no_grad():
        last_value, _ = agent.get_value(state, critic_hidden)

    # è®¡ç®— GAE å’Œ Returns
    buffer.compute_returns_and_advantages(torch.tensor([last_value]).to(device), config.GAMMA, config.LMBDA)

    # æ‰§è¡Œ PPO æ›´æ–°
    agent.update(buffer, writer, global_step)

    # --- 3. è®°å½•æ—¥å¿— (Rollout çº§åˆ«) ---
    sps = int(global_step / (time.time() - start_time))
    writer.add_scalar("Metrics/SPS (Steps Per Second)", sps, global_step)

    if len(ep_rewards) > 0:  # åªæœ‰åœ¨ rollout ä¸­æœ‰ episode ç»“æŸæ—¶æ‰è®°å½•
        writer.add_scalar("Episode/Mean_Reward", np.mean(ep_rewards), global_step)
        writer.add_scalar("Episode/Mean_Success_Rate", np.mean(ep_successes), global_step)
        writer.add_scalar("Episode/Mean_Length", np.mean(ep_lengths), global_step)

        # ğŸŒŸ ä¿®å¤ 5: åœ¨æ­¤å¤„æ·»åŠ æ‰€æœ‰å¥–åŠ±åˆ†é¡¹çš„æ—¥å¿—
        writer.add_scalar("Reward_Components/Mean_Success", np.mean(ep_rew_success), global_step)
        writer.add_scalar("Reward_Components/Mean_Collision", np.mean(ep_rew_collision), global_step)
        writer.add_scalar("Reward_Components/Mean_Shaping", np.mean(ep_rew_shaping), global_step)
        writer.add_scalar("Reward_Components/Mean_Heading", np.mean(ep_rew_heading), global_step)
        writer.add_scalar("Reward_Components/Mean_Obstacle", np.mean(ep_rew_obstacle), global_step)
        writer.add_scalar("Reward_Components/Mean_Step_Penalty", np.mean(ep_rew_step), global_step)

    # æ¯ 100 æ¬¡æ›´æ–°ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    if update_num % 100 == 0:
        torch.save(agent.actor.state_dict(), f'gru_ppo_actor_{update_num}.pth')
        torch.save(agent.critic.state_dict(), f'gru_ppo_critic_{update_num}.pth')

# --- è®­ç»ƒç»“æŸ ---
writer.close()
print("Training finished. Saving final models.")
torch.save(agent.actor.state_dict(), 'gru_ppo_actor_dynamic_final.pth')
torch.save(agent.critic.state_dict(), 'gru_ppo_critic_dynamic_final.pth')